{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVqc1lGgvr1A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from datasets import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8NMxgRwvr1D"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
        "train_data = pd.read_csv('data/imdb_train.csv',index_col='Unnamed: 0')\n",
        "test_data = pd.read_csv('data/imdb_test.csv',index_col='Unnamed: 0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "T0iiYTedAr0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuTtM-blvr1E"
      },
      "outputs": [],
      "source": [
        "X_train = train_data['text']\n",
        "X_test = test_data['text']\n",
        "y_train = train_data['label']\n",
        "y_test = test_data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT7NUd36vr1E"
      },
      "outputs": [],
      "source": [
        "# LabelEncoding –¥–ª—è label\n",
        "ratings = list(range(1, 5)) + list(range(7, 11))\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(ratings)\n",
        "\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhLShiQ5vr1F"
      },
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
        "train_dataset = MovieReviewDataset(X_train.tolist(), y_train.tolist())\n",
        "test_dataset = MovieReviewDataset(X_test.tolist(), y_test.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac-2eOGVvr1F"
      },
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∫ –≤–∞—à–∏–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fq--me_vr1F",
        "outputId": "9a18400b-20e7-42e8-877b-824b95b447ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)  # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ num_labels=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaYPaXolvr1I"
      },
      "outputs": [],
      "source": [
        "# –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç), –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏\n",
        "train_dataset = train_dataset.remove_columns(['text'])\n",
        "test_dataset = test_dataset.remove_columns(['text'])\n",
        "\n",
        "# –£–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –º–µ—Ç–∫–∏ –Ω—É–∂–Ω—ã –≤ –≤–∏–¥–µ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª\n",
        "train_dataset.set_format('torch')\n",
        "test_dataset.set_format('torch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxCP-zsHvr1J",
        "outputId": "72597f79-f001-48c3-bf86-d643bb652b19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/userr/projects/greenatom/.venv/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='maybe_worked',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='maybe_worked_logs',\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_steps=10,\n",
        "    save_strategy='epoch'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmIKVqIyvr1K"
      },
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º Trainer —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,  # –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
        "    compute_metrics=lambda p: {\n",
        "        'mae': mean_absolute_error(p.label_ids, p.predictions.flatten())\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2JJIvH7vr1K",
        "outputId": "98f4e138-b4c3-4eb3-83d9-64a2daafb2aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4689/4689 13:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mae</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.263900</td>\n",
              "      <td>6.038891</td>\n",
              "      <td>1.620651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.213400</td>\n",
              "      <td>4.454584</td>\n",
              "      <td>1.392040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.810700</td>\n",
              "      <td>4.276865</td>\n",
              "      <td>1.360022</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4689, training_loss=1.111080771495244, metrics={'train_runtime': 830.0073, 'train_samples_per_second': 90.361, 'train_steps_per_second': 5.649, 'total_flos': 2483719430400000.0, 'train_loss': 1.111080771495244, 'epoch': 3.0})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkqQ8pcyvr1L",
        "outputId": "ecdeb3d5-1fe7-4a3f-f0bd-fe13241314ee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1563/1563 01:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 4.1326470375061035,\n",
              " 'eval_mae': 1.3495981693267822,\n",
              " 'eval_runtime': 64.8171,\n",
              " 'eval_samples_per_second': 385.701,\n",
              " 'eval_steps_per_second': 24.114,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# –û—Ü–µ–Ω–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "test_dataset = MovieReviewDataset(test_data['text'].tolist(), test_data['label'].tolist())\n",
        "trainer.evaluate(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫"
      ],
      "metadata": {
        "id": "_pCTNj09_XTx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtZf0ScZvr1L",
        "outputId": "f93bfc1e-d065-4c96-c1a0-be6c652352a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "105_Aj0Wvr1M"
      },
      "outputs": [],
      "source": [
        "def predict(text):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É.\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - text (str): –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - int: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É\n",
        "    \"\"\"\n",
        "    text = split_long_text(text)\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    if 'token_type_ids' in inputs:\n",
        "        del inputs['token_type_ids']\n",
        "\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "\n",
        "    score = round(outputs.logits.item())\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oT_3XiKvr1N"
      },
      "outputs": [],
      "source": [
        "def split_long_text(text, max_length=100):\n",
        "    \"\"\"\n",
        "    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ–¥–µ.\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - text (str): –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "    - max_length (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - str: —Å—Ç—Ä–æ–∫–∞, —Ä–∞–∑–±–∏—Ç–∞—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π, –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é —Å–∏–º–≤–æ–ª–∞ \"+\"\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        if len(' '.join(current_line + [word])) > max_length:\n",
        "            lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "        else:\n",
        "            current_line.append(word)\n",
        "\n",
        "    if current_line:\n",
        "        lines.append(' '.join(current_line))\n",
        "\n",
        "    return ' +\\n'.join([f'\"{line}\"' for line in lines])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itOldTgVvr1O",
        "outputId": "557e34c7-3b77-4da1-93a1-31dc9167db96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('maybe_final_project/tokenizer_config.json',\n",
              " 'maybe_final_project/special_tokens_map.json',\n",
              " 'maybe_final_project/vocab.txt',\n",
              " 'maybe_final_project/added_tokens.json',\n",
              " 'maybe_final_project/tokenizer.json')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# –ß—Ç–æ–±—ã —Å–ª—É—á–∞–π–Ω–æ –Ω–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
        "trainer.save_model(\"final_project\")\n",
        "tokenizer.save_pretrained(\"final_project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-QtsjpGvr1P",
        "outputId": "83c325cd-3f62-47c5-858c-ab628709b6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Rating: 1\n"
          ]
        }
      ],
      "source": [
        "predicted_rating = predict(\n",
        "    \"The first time I watched this show it was OK. There were some funny moments and I laughed a couple of times but \"\n",
        "    \"this show is getting worse and worse. Carly and Sam's web show is NOT the least bit funny. They play a stupid video \"\n",
        "    \"from the internet, scream at the camera and make some very bad jokes. And then the laugh track goes off?! One problem \"\n",
        "    \"with the show is that none of the main characters are funny. Carly is not funny. Miranda Cosgrove's acting is lackluster \"\n",
        "    \"at best. Her acting in this show is nothing like her acting from Drake And Josh. Her friend Sam is very rude and crude \"\n",
        "    \"and the show is written in a way that makes her look like some kind of hillbilly. I mean they make jokes about her mom \"\n",
        "    \"driving a rusty old truck, her mom smashing an old TV with a bat, and then there's the jokes about Sam failing in school, \"\n",
        "    \"getting detention all the time and running from cops. None of that is funny at all. Then there's Freddy who is a computer \"\n",
        "    \"geek. He isn't too funny unless his Mom is treating him like a baby. The show's only somewhat funny full time character \"\n",
        "    \"is Carly's brother Spencer. He makes some funny jokes and does some pretty funny things like pretending to drive a spaceship \"\n",
        "    \"while making spaceship noises, knocking over a girl scouts' cookie table for revenge as they did the same thing to him. \"\n",
        "    \"His material is the only thing worth laughing at. Aside from the characters other things make the show bad too. Like the fact \"\n",
        "    \"that a couple of kids doing a local web show from a Seattle apartment is a worldwide hit and got them a free trip to Tokyo? \"\n",
        "    \"Another thing is that how can a 26 year old single guy with no real job can pay for a 2 level apartment in downtown Seattle \"\n",
        "    \"and raise his 13 year old sister and pay for a room full of camera and sound equipment including a remote controlled projector \"\n",
        "    \"and a green screen and an HD camera? This sounds like it was written by a 10 year old. The worst thing is that the show contains \"\n",
        "    \"some pretty questionable content. There are a couple of times when Carly(remember a 13 year old girl) appears on her internet web \"\n",
        "    \"show in a bikini top. WTF? Then I saw an episode where Freddy tells Carly and Sam that he 'slept in JUST his socks the night before.' \"\n",
        "    \"I mean WTF? Then there's an episode where Carly's rival Nevel blackmails her by taking her website rights and agrees to give her the \"\n",
        "    \"website back in exchange for a kiss. Creepy! And I just saw an episode where Carly meets a boy who just moved into their apartment building \"\n",
        "    \"and he has some kind of back injury and he takes off his shirt and Carly stands there drooling over him. I can't believe Nick even lets them \"\n",
        "    \"show that kind of stuff and I can't believe that this was created by the same guy responsible for Drake and Josh. This show is not appropriate \"\n",
        "    \"for kids under the age of 12 and that's even questionable. iCarly is just another addition to the long list of awful Nick programming.\"\n",
        ")\n",
        "\n",
        "print(f\"Predicted Rating: {predicted_rating}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}